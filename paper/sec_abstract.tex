\begin{abstract}
    Obtaining deep networks that are robust against adversarial examples \emph{and} generalize well is an open problem. A recent hypothesis \cite{TsiprasARXIV2018,SuARXV2018} even states that \emph{both} robust \emph{and} accurate models are impossible, \ie, adversarial robustness and generalization are conflicting goals. In an effort to clarify the relationship between robustness and generalization, we assume an underlying, low-dimensional data manifold and show that:
	\begin{enumerate*}
    	\item regular adversarial examples leave the manifold;
    	\item adversarial examples constrained to the manifold, \ie, on-manifold adversarial examples, exist;
    	\item on-manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization;
    	\item \red{regular robustness and generalization are not necessarily contradicting goals.}
	\end{enumerate*}
    These assumptions imply that \emph{both} robust \emph{and} accurate models are possible. However, different models (architectures, training strategies \etc) can exhibit different robustness and generalization characteristics. To confirm our claims, we present extensive experiments on synthetic data (with known manifold) as well as on \MNIST \cite{CohenARXIV2017}, \FashionMNIST \cite{XiaoARXIV2017} and \Celeb \cite{LiuICCV2015}.
\end{abstract}