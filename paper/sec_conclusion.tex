\section{Conclusion}
\label{sec:conclusion}

In this paper, we intended to disentangle the relationship between adversarial robustness and generalization by initially adopting the hypothesis that robustness and generalization are contradictory \cite{TsiprasARXIV2018,SuARXV2018}. By considering adversarial examples in the context of the low-dimensional, underlying data manifold, we formulated and experimentally confirmed four assumptions. First, we showed that regular adversarial examples indeed leave the manifold, as widely assumed in related work \cite{GilmerICLRWORK2018,TanayARXIV2016,IlyasARXIV2017,SamangoueiICLR2018,SchottARXIV2018}. Second, we demonstrated that adversarial examples can also be found on the manifold, so-called on-manifold adversarial examples; even if the manifold has to be approximated, \eg, using \VAEGANs \cite{LarsenICML2016,RoscaARXIV2017}. Third, we established that robustness against on-manifold adversarial examples is clearly related to generalization. Our proposed on-manifold adversarial training exploits this relationship to boost generalization using an approximate manifold, or known invariances. Fourth, we provided evidence that \red{robustness against regular, unconstrained adversarial examples and generalization are not necessarily contradicting goals}: for any arbitrary but fixed model, better generalization, \eg, through more training data, does not reduce robustness.